{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, roc_auc_score, precision_score, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annot_classification_correspondance(classif: int) -> int:\n",
    "\n",
    "    if classif == 2 or classif == 3:\n",
    "        classif = 0\n",
    "\n",
    "    return classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      frame  ecg_signal  cons  anno1  anno2  anno3  ml cnn  target  \\\n",
       "0  57600001        -373   1.0    1.0    1.0      1   0   1     1.0   \n",
       "1  57600002        -361   1.0    1.0    1.0      1   0   1     1.0   \n",
       "2  57600003        -341   1.0    1.0    1.0      1   0   1     1.0   \n",
       "3  57600004        -317   1.0    1.0    1.0      1   0   1     1.0   \n",
       "4  57600005        -297   1.0    1.0    1.0      1   0   1     1.0   \n",
       "\n",
       "   ml_precision  cnn_precision  \n",
       "0             0              1  \n",
       "1             0              1  \n",
       "2             0              1  \n",
       "3             0              1  \n",
       "4             0              1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>frame</th>\n      <th>ecg_signal</th>\n      <th>cons</th>\n      <th>anno1</th>\n      <th>anno2</th>\n      <th>anno3</th>\n      <th>ml</th>\n      <th>cnn</th>\n      <th>target</th>\n      <th>ml_precision</th>\n      <th>cnn_precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>57600001</td>\n      <td>-373</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>57600002</td>\n      <td>-361</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57600003</td>\n      <td>-341</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>57600004</td>\n      <td>-317</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>57600005</td>\n      <td>-297</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "df_results = pd.read_csv('../dataset_streamlit/df_model_comparison_103001_selection.csv')\n",
    "df_results = df_results.rename(columns={'Unnamed: 0': 'frame'}) \n",
    "df_results['target'] = df_results['cons'].apply(lambda x: annot_classification_correspondance(x))\n",
    "df_results['ml_precision'] = (df_results['ml'] == df_results['target']).apply(lambda x: 1 if x is True else 0)\n",
    "df_results['cnn_precision'] = (df_results['cnn'] == df_results['target']).apply(lambda x: 1 if x is True else 0)\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19.994416666666666"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# Temps de l'enregistrement \n",
    "df_results.shape[0]/1000/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# Some values are '1' and 'na'\n",
    "def clean_cnn_values(x):\n",
    "    if x == '1':\n",
    "        x = 1\n",
    "    elif x ==  'na':\n",
    "        x = 0\n",
    "    return x\n",
    "df_results['cnn'] = df_results['cnn'].apply(lambda x: clean_cnn_values(x))\n",
    "\n",
    "df_results['cnn'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0    0.539\n1.0    0.461\nName: target, dtype: float64\n\n\n1    0.597\n0    0.403\nName: ml_precision, dtype: float64\n\n\n0    0.579\n1    0.421\nName: cnn_precision, dtype: float64\n\n\n"
     ]
    }
   ],
   "source": [
    "for column in ['target', 'ml_precision', 'cnn_precision']:\n",
    "    print(round(df_results[column].value_counts()/df_results.shape[0],3))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "f1-score on ML : 0.3781434951643479\n",
      "f1-score on  : 0.5259880730652905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    print()\n",
    "    print(\"f1-score on ML :\", f1_score(df_results['ml'], df_results['target']))\n",
    "    print(\"f1-score on  :\", f1_score(df_results['cnn'], df_results['target']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_performance_report(y_pred, y_true):\n",
    "\n",
    "    print('\\nPerformance report :\\n')\n",
    "\n",
    "    # Creation of the predictions of y_train & y_test\n",
    "\n",
    "    # Output the accuracy and f1 score for the model\n",
    "    print()\n",
    "    print(\"Accuracy on train set :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"f1-score on train set :\", f1_score(y_true, y_pred))\n",
    "    print(\"Recall on train set :\", recall_score(y_true, y_pred))\n",
    "    print(\"Precision on train set :\", precision_score(y_true, y_pred))\n",
    "    print(\"ROC_AUC_score on train set :\", roc_auc_score(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "    # # Plot non-normalized confusion matrix\n",
    "    # titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "    #                 (\"Normalized confusion matrix\", 'true')]\n",
    "    # for title, normalize in titles_options:\n",
    "    #     disp = plot_confusion_matrix(model, X_test, y_test,\n",
    "    #                                 display_labels=[0,1],\n",
    "    #                                 cmap=plt.cm.Blues,\n",
    "    #                                 normalize=normalize)\n",
    "    #     disp.ax_.set_title(title)\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Performance report :\n",
      "\n",
      "\n",
      "Accuracy on train set : 0.5966782393418163\n",
      "f1-score on train set : 0.3781434951643479\n",
      "Recall on train set : 0.2659892419653754\n",
      "Precision on train set : 0.6538311111111111\n",
      "ROC_AUC_score on train set : 0.5727647999214278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_performance_report(df_results['ml'].values, df_results['target'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Performance report :\n",
      "\n",
      "\n",
      "Accuracy on train set : 0.42190944972138056\n",
      "f1-score on train set : 0.5259880730652905\n",
      "Recall on train set : 0.6957103466980066\n",
      "Precision on train set : 0.4228351648351648\n",
      "ROC_AUC_score on train set : 0.4417090838641675\n",
      "\n"
     ]
    }
   ],
   "source": [
    "make_performance_report(df_results['cnn'].values, df_results['target'].values)"
   ]
  }
 ]
}